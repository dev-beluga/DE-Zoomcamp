{"payload":{"allShortcutsEnabled":false,"fileTree":{"cohorts/2024/workshops/dlt_resources":{"items":[{"name":"data_ingestion_workshop.md","path":"cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md","contentType":"file"},{"name":"homework_starter.ipynb","path":"cohorts/2024/workshops/dlt_resources/homework_starter.ipynb","contentType":"file"},{"name":"incremental_loading.png","path":"cohorts/2024/workshops/dlt_resources/incremental_loading.png","contentType":"file"},{"name":"workshop.ipynb","path":"cohorts/2024/workshops/dlt_resources/workshop.ipynb","contentType":"file"}],"totalCount":4},"cohorts/2024/workshops":{"items":[{"name":"dlt_resources","path":"cohorts/2024/workshops/dlt_resources","contentType":"directory"},{"name":"dlt.md","path":"cohorts/2024/workshops/dlt.md","contentType":"file"},{"name":"rising-wave.md","path":"cohorts/2024/workshops/rising-wave.md","contentType":"file"}],"totalCount":3},"cohorts/2024":{"items":[{"name":"01-docker-terraform","path":"cohorts/2024/01-docker-terraform","contentType":"directory"},{"name":"02-workflow-orchestration","path":"cohorts/2024/02-workflow-orchestration","contentType":"directory"},{"name":"03-data-warehouse","path":"cohorts/2024/03-data-warehouse","contentType":"directory"},{"name":"04-analytics-engineering","path":"cohorts/2024/04-analytics-engineering","contentType":"directory"},{"name":"workshops","path":"cohorts/2024/workshops","contentType":"directory"},{"name":"README.md","path":"cohorts/2024/README.md","contentType":"file"},{"name":"project.md","path":"cohorts/2024/project.md","contentType":"file"}],"totalCount":7},"cohorts":{"items":[{"name":"2022","path":"cohorts/2022","contentType":"directory"},{"name":"2023","path":"cohorts/2023","contentType":"directory"},{"name":"2024","path":"cohorts/2024","contentType":"directory"}],"totalCount":3},"":{"items":[{"name":".devcontainer","path":".devcontainer","contentType":"directory"},{"name":"01-docker-terraform","path":"01-docker-terraform","contentType":"directory"},{"name":"02-workflow-orchestration","path":"02-workflow-orchestration","contentType":"directory"},{"name":"03-data-warehouse","path":"03-data-warehouse","contentType":"directory"},{"name":"04-analytics-engineering","path":"04-analytics-engineering","contentType":"directory"},{"name":"05-batch","path":"05-batch","contentType":"directory"},{"name":"06-streaming","path":"06-streaming","contentType":"directory"},{"name":"cohorts","path":"cohorts","contentType":"directory"},{"name":"images","path":"images","contentType":"directory"},{"name":"projects","path":"projects","contentType":"directory"},{"name":".gitignore","path":".gitignore","contentType":"file"},{"name":"README.md","path":"README.md","contentType":"file"},{"name":"after-sign-up.md","path":"after-sign-up.md","contentType":"file"},{"name":"asking-questions.md","path":"asking-questions.md","contentType":"file"},{"name":"cerficates.md","path":"cerficates.md","contentType":"file"},{"name":"dataset.md","path":"dataset.md","contentType":"file"}],"totalCount":16}},"fileTreeProcessingTime":10.353232,"foldersToFetch":[],"repo":{"id":419661684,"defaultBranch":"main","name":"data-engineering-zoomcamp","ownerLogin":"DataTalksClub","currentUserCanPush":false,"isFork":false,"isEmpty":false,"createdAt":"2021-10-21T09:32:50.000Z","ownerAvatar":"https://avatars.githubusercontent.com/u/72699292?v=4","public":true,"private":false,"isOrgOwned":true},"symbolsExpanded":false,"treeExpanded":true,"refInfo":{"name":"main","listCacheKey":"v0:1707083256.0","canEdit":false,"refType":"branch","currentOid":"1c7926a713205d6bc1456f7f446208115d06050b"},"path":"cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md","currentUser":null,"blob":{"rawLines":null,"stylingDirectives":null,"csv":null,"csvError":null,"dependabotInfo":{"showConfigurationBanner":false,"configFilePath":null,"networkDependabotPath":"/DataTalksClub/data-engineering-zoomcamp/network/updates","dismissConfigurationNoticePath":"/settings/dismiss-notice/dependabot_configuration_notice","configurationNoticeDismissed":null,"repoAlertsPath":"/DataTalksClub/data-engineering-zoomcamp/security/dependabot","repoSecurityAndAnalysisPath":"/DataTalksClub/data-engineering-zoomcamp/settings/security_analysis","repoOwnerIsOrg":true,"currentUserCanAdminRepo":false},"displayName":"data_ingestion_workshop.md","displayUrl":"https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md?raw=true","headerInfo":{"blobSize":"26.7 KB","deleteInfo":{"deleteTooltip":"You must be signed in to make or propose changes"},"editInfo":{"editTooltip":"You must be signed in to make or propose changes"},"ghDesktopPath":"https://desktop.github.com","gitLfsPath":null,"onBranch":true,"shortPath":"c1eeb20","siteNavLoginPath":"/login?return_to=https%3A%2F%2Fgithub.com%2FDataTalksClub%2Fdata-engineering-zoomcamp%2Fblob%2Fmain%2Fcohorts%2F2024%2Fworkshops%2Fdlt_resources%2Fdata_ingestion_workshop.md","isCSV":false,"isRichtext":true,"toc":[{"level":1,"text":"Intro","anchor":"intro","htmlText":"Intro"},{"level":3,"text":"‚ÄúA wild dataset magically appears!‚Äù","anchor":"a-wild-dataset-magically-appears","htmlText":"‚ÄúA wild dataset magically appears!‚Äù"},{"level":3,"text":"Be the magician! üòé","anchor":"be-the-magician-","htmlText":"Be the magician! üòé"},{"level":3,"text":"What else does a data engineer do? What are we not learning, and what are we learning?","anchor":"what-else-does-a-data-engineer-do-what-are-we-not-learning-and-what-are-we-learning","htmlText":"What else does a data engineer do? What are we not learning, and what are we learning?"},{"level":1,"text":"Extracting data","anchor":"extracting-data","htmlText":"Extracting data"},{"level":3,"text":"The considerations of extracting data","anchor":"the-considerations-of-extracting-data","htmlText":"The considerations of extracting data"},{"level":3,"text":"Extracting data without hitting hardware limits","anchor":"extracting-data-without-hitting-hardware-limits","htmlText":"Extracting data without hitting hardware limits"},{"level":3,"text":"Managing memory.","anchor":"managing-memory","htmlText":"Managing memory."},{"level":3,"text":"So how do we avoid filling the memory?","anchor":"so-how-do-we-avoid-filling-the-memory","htmlText":"So how do we avoid filling the memory?"},{"level":3,"text":"Control the max memory used by streaming the data","anchor":"control-the-max-memory-used-by-streaming-the-data","htmlText":"Control the max memory used by streaming the data"},{"level":3,"text":"Streaming in python via generators","anchor":"streaming-in-python-via-generators","htmlText":"Streaming in python via generators"},{"level":3,"text":"Generator examples:","anchor":"generator-examples","htmlText":"Generator examples:"},{"level":2,"text":"3 Extraction examples:","anchor":"3-extraction-examples","htmlText":"3 Extraction examples:"},{"level":3,"text":"Example 1: Grabbing data from an api","anchor":"example-1-grabbing-data-from-an-api","htmlText":"Example 1: Grabbing data from an api"},{"level":3,"text":"Example 2: Grabbing the same data from file - simple download","anchor":"example-2-grabbing-the-same-data-from-file---simple-download","htmlText":"Example 2: Grabbing the same data from file - simple download"},{"level":3,"text":"Example 3: Same file, streaming download","anchor":"example-3-same-file-streaming-download","htmlText":"Example 3: Same file, streaming download"},{"level":1,"text":"Normalising data","anchor":"normalising-data","htmlText":"Normalising data"},{"level":3,"text":"Part of what we often call data cleaning is just metadata work:","anchor":"part-of-what-we-often-call-data-cleaning-is-just-metadata-work","htmlText":"Part of what we often call data cleaning is just metadata work:"},{"level":3,"text":"Why prepare data? why not use json as is?","anchor":"why-prepare-data-why-not-use-json-as-is","htmlText":"Why prepare data? why not use json as is?"},{"level":3,"text":"Practical example","anchor":"practical-example","htmlText":"Practical example"},{"level":2,"text":"Introducing dlt","anchor":"introducing-dlt","htmlText":"Introducing dlt"},{"level":1,"text":"Incremental loading","anchor":"incremental-loading","htmlText":"Incremental loading"},{"level":3,"text":"dlt currently supports 2 ways of loading incrementally:","anchor":"dlt-currently-supports-2-ways-of-loading-incrementally","htmlText":"dlt currently supports 2 ways of loading incrementally:"},{"level":3,"text":"Let‚Äôs do a merge example together:","anchor":"lets-do-a-merge-example-together","htmlText":"Let‚Äôs do a merge example together:"},{"level":3,"text":"What‚Äôs next?","anchor":"whats-next","htmlText":"What‚Äôs next?"}],"lineInfo":{"truncatedLoc":"582","truncatedSloc":"413"},"mode":"file"},"image":false,"isCodeownersFile":null,"isPlain":false,"isValidLegacyIssueTemplate":false,"issueTemplateHelpUrl":"https://docs.github.com/articles/about-issue-and-pull-request-templates","issueTemplate":null,"discussionTemplate":null,"language":"Markdown","languageID":222,"large":false,"loggedIn":false,"newDiscussionPath":"/DataTalksClub/data-engineering-zoomcamp/discussions/new","newIssuePath":"/DataTalksClub/data-engineering-zoomcamp/issues/new","planSupportInfo":{"repoIsFork":null,"repoOwnedByCurrentUser":null,"requestFullPath":"/DataTalksClub/data-engineering-zoomcamp/blob/main/cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md","showFreeOrgGatedFeatureMessage":null,"showPlanSupportBanner":null,"upgradeDataAttributes":null,"upgradePath":null},"publishBannersInfo":{"dismissActionNoticePath":"/settings/dismiss-notice/publish_action_from_dockerfile","releasePath":"/DataTalksClub/data-engineering-zoomcamp/releases/new?marketplace=true","showPublishActionBanner":false},"rawBlobUrl":"https://github.com/DataTalksClub/data-engineering-zoomcamp/raw/main/cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md","renderImageOrRaw":false,"richText":"<article class=\"markdown-body entry-content container-lg\" itemprop=\"text\"><h1 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-intro\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#intro\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Intro</h1>\n<p dir=\"auto\">What is data loading, or data ingestion?</p>\n<p dir=\"auto\">Data ingestion is the process of extracting data from a producer, transporting it to a convenient environment, and preparing it for usage by normalising it, sometimes cleaning, and adding metadata.</p>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-a-wild-dataset-magically-appears\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#a-wild-dataset-magically-appears\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>‚ÄúA wild dataset magically appears!‚Äù</h3>\n<p dir=\"auto\">In many data science teams, data magically appears - because the engineer loads it.</p>\n<ul dir=\"auto\">\n<li>Sometimes the format in which it appears is structured, and with explicit schema\n<ul dir=\"auto\">\n<li>In that case, they can go straight to using it; Examples: Parquet, Avro, or table in a db,</li>\n</ul>\n</li>\n<li>Sometimes the format is weakly typed and without explicit schema, such as csv, json\n<ul dir=\"auto\">\n<li>in which case some extra normalisation or cleaning might be needed before usage</li>\n</ul>\n</li>\n</ul>\n<blockquote>\n<p dir=\"auto\">üí° <strong>What is a schema?</strong> The schema specifies the expected format and structure of data within a document or data store, defining the allowed keys, their data types, and any constraints or relationships.</p>\n</blockquote>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-be-the-magician-\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#be-the-magician-\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Be the magician! üòé</h3>\n<p dir=\"auto\">Since you are here to learn about data engineering, you will be the one making datasets magically appear.</p>\n<p dir=\"auto\">Here‚Äôs what you need to learn to build pipelines</p>\n<ul dir=\"auto\">\n<li>Extracting data</li>\n<li>Normalising, cleaning, adding metadata such as schema and types</li>\n<li>and Incremental loading, which is vital for fast, cost effective data refreshes.</li>\n</ul>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-what-else-does-a-data-engineer-do-what-are-we-not-learning-and-what-are-we-learning\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#what-else-does-a-data-engineer-do-what-are-we-not-learning-and-what-are-we-learning\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>What else does a data engineer do? What are we not learning, and what are we learning?</h3>\n<ul dir=\"auto\">\n<li>It might seem simplistic, but in fact a data engineer‚Äôs main goal is to ensure data flows from source systems to analytical destinations.</li>\n<li>So besides building pipelines, running pipelines and fixing pipelines, a data engineer may also focus on optimising data storage, ensuring data quality and integrity, implementing effective data governance practices, and continuously refining data architecture to meet the evolving needs of the organisation.</li>\n<li>Ultimately, a data engineer's role extends beyond the mechanical aspects of pipeline development, encompassing the strategic management and enhancement of the entire data lifecycle.</li>\n<li>This workshop focuses on building robust, scalable, self maintaining pipelines, with built in governance - in other words, best practices applied.</li>\n</ul>\n<h1 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-extracting-data\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#extracting-data\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Extracting data</h1>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-the-considerations-of-extracting-data\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#the-considerations-of-extracting-data\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>The considerations of extracting data</h3>\n<p dir=\"auto\">In this section we will learn about extracting data from source systems, and what to care about when doing so.</p>\n<p dir=\"auto\">Most data is stored behind an API</p>\n<ul dir=\"auto\">\n<li>Sometimes that‚Äôs a RESTful api for some business application, returning records of data.</li>\n<li>Sometimes the API returns a secure file path to something like a json or parquet file in a bucket that enables you to grab the data in bulk,</li>\n<li>Sometimes the API is something else (mongo, sql, other databases or applications) and will generally return records as JSON - the most common interchange format.</li>\n</ul>\n<p dir=\"auto\">As an engineer, you will need to build pipelines that ‚Äújust work‚Äù.</p>\n<p dir=\"auto\">So here‚Äôs what you need to consider on extraction, to prevent the pipelines from breaking, and to keep them running smoothly.</p>\n<ul dir=\"auto\">\n<li>Hardware limits: During this course we will cover how to navigate the challenges of managing memory.</li>\n<li>Network limits: Sometimes networks can fail. We can‚Äôt fix what could go wrong but we can retry network jobs until they succeed. For example, dlt library offers a requests ‚Äúreplacement‚Äù that has built in retries. <a href=\"https://dlthub.com/docs/reference/performance#using-the-built-in-requests-client\" rel=\"nofollow\">Docs</a>. We won‚Äôt focus on this during the course but you can read the docs on your own.</li>\n<li>Source api limits: Each source might have some limits such as how many requests you can do per second. We would call these ‚Äúrate limits‚Äù. Read each source‚Äôs docs carefully to understand how to navigate these obstacles. You can find some examples of how to wait for rate limits in our verified sources repositories\n<ul dir=\"auto\">\n<li>examples: <a href=\"https://developer.zendesk.com/api-reference/introduction/rate-limits/\" rel=\"nofollow\">Zendesk</a>, <a href=\"https://shopify.dev/docs/api/usage/rate-limits\" rel=\"nofollow\">Shopify</a></li>\n</ul>\n</li>\n</ul>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-extracting-data-without-hitting-hardware-limits\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#extracting-data-without-hitting-hardware-limits\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Extracting data without hitting hardware limits</h3>\n<p dir=\"auto\">What kind of limits could you hit on your machine? In the case of data extraction, the only limits are memory and storage. This refers to the RAM or virtual memory, and the disk, or physical storage.</p>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-managing-memory\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#managing-memory\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a><strong>Managing memory.</strong></h3>\n<ul dir=\"auto\">\n<li>Many data pipelines run on serverless functions or on orchestrators that delegate the workloads to clusters of small workers.</li>\n<li>These systems have a small memory or share it between multiple workers - so filling the memory is BAAAD: It might lead to not only your pipeline crashing, but crashing the entire container or machine that might be shared with other worker processes, taking them down too.</li>\n<li>The same can be said about disk - in most cases your disk is sufficient, but in some cases it‚Äôs not. For those cases, mounting an external drive mapped to a storage bucket is the way to go. Airflow for example supports a ‚Äúdata‚Äù folder that is used just like a local folder but can be mapped to a bucket for unlimited capacity.</li>\n</ul>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-so-how-do-we-avoid-filling-the-memory\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#so-how-do-we-avoid-filling-the-memory\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>So how do we avoid filling the memory?</h3>\n<ul dir=\"auto\">\n<li>We often do not know the volume of data upfront</li>\n<li>And we cannot scale dynamically or infinitely on hardware during runtime</li>\n<li>So the answer is: Control the max memory you use</li>\n</ul>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-control-the-max-memory-used-by-streaming-the-data\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#control-the-max-memory-used-by-streaming-the-data\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Control the max memory used by streaming the data</h3>\n<p dir=\"auto\">Streaming here refers to processing the data event by event or chunk by chunk instead of doing bulk operations.</p>\n<p dir=\"auto\">Let‚Äôs look at some classic examples of streaming where data is transferred chunk by chunk or event by event</p>\n<ul dir=\"auto\">\n<li>Between an audio broadcaster and an in-browser audio player</li>\n<li>Between a server and a local video player</li>\n<li>Between a smart home device or IoT device and your phone</li>\n<li>between google maps and your navigation app</li>\n<li>Between instagram live and your followers</li>\n</ul>\n<p dir=\"auto\">What do data engineers do? We usually stream the data between buffers, such as</p>\n<ul dir=\"auto\">\n<li>from API to local file</li>\n<li>from webhooks to event queues</li>\n<li>from event queue (Kafka, SQS) to Bucket</li>\n</ul>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-streaming-in-python-via-generators\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#streaming-in-python-via-generators\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Streaming in python via generators</h3>\n<p dir=\"auto\">Let‚Äôs focus on how we build most data pipelines:</p>\n<ul dir=\"auto\">\n<li>To process data in a stream in python, we use generators, which are functions that can return multiple times - by allowing multiple returns, the data can be released as it‚Äôs produced, as stream, instead of returning it all at once as a batch.</li>\n</ul>\n<p dir=\"auto\">Take the following theoretical example:</p>\n<ul dir=\"auto\">\n<li>We search twitter for ‚Äúcat pictures‚Äù. We do not know how many pictures will be returned - maybe 10, maybe 10.000.000. Will they fit in memory? Who knows.</li>\n<li>So to grab this data without running out of memory, we would use a python generator.</li>\n<li>What‚Äôs a generator? In simple words, it‚Äôs a function that can return multiple times. Here‚Äôs an example of a regular function, and how that function looks if written as a generator.</li>\n</ul>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-generator-examples\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#generator-examples\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Generator examples:</h3>\n<p dir=\"auto\">Let‚Äôs look at a regular returning function, and how we can re-write it as a generator.</p>\n<p dir=\"auto\"><strong>Regular function collects data in memory.</strong> Here you can see how data is collected row by row in a list called <code>data</code>before it is returned. This will break if we have more data than memory.</p>\n<div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"def search_twitter(query):\n\tdata = []\n\tfor row in paginated_get(query):\n\t\tdata.append(row)\n\treturn data\n\n# Collect all the cat picture data\nfor row in search_twitter(&quot;cat pictures&quot;):\n  # Once collected, \n  # print row by row\n\tprint(row)\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">search_twitter</span>(<span class=\"pl-s1\">query</span>):\n\t<span class=\"pl-s1\">data</span> <span class=\"pl-c1\">=</span> []\n\t<span class=\"pl-k\">for</span> <span class=\"pl-s1\">row</span> <span class=\"pl-c1\">in</span> <span class=\"pl-en\">paginated_get</span>(<span class=\"pl-s1\">query</span>):\n\t\t<span class=\"pl-s1\">data</span>.<span class=\"pl-en\">append</span>(<span class=\"pl-s1\">row</span>)\n\t<span class=\"pl-k\">return</span> <span class=\"pl-s1\">data</span>\n\n<span class=\"pl-c\"># Collect all the cat picture data</span>\n<span class=\"pl-k\">for</span> <span class=\"pl-s1\">row</span> <span class=\"pl-c1\">in</span> <span class=\"pl-en\">search_twitter</span>(<span class=\"pl-s\">\"cat pictures\"</span>):\n  <span class=\"pl-c\"># Once collected, </span>\n  <span class=\"pl-c\"># print row by row</span>\n\t<span class=\"pl-en\">print</span>(<span class=\"pl-s1\">row</span>)</pre></div>\n<p dir=\"auto\">When calling <code>for row in search_twitter(\"cat pictures\"):</code> all the data must first be downloaded before the first record is returned</p>\n<p dir=\"auto\">Let‚Äôs see how we could rewrite this as a generator.</p>\n<p dir=\"auto\"><strong>Generator for streaming the data.</strong> The memory usage here is minimal.</p>\n<p dir=\"auto\">As you can see, in the modified function, we yield each row as we get the data, without collecting it into memory. We can then run this generator and handle the data item by item.</p>\n<div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"def search_twitter(query):\n\tfor row in paginated_get(query):\n\t\tyield row\n\n# Get one row at a time\nfor row in extract_data(&quot;cat pictures&quot;):\n\t# print the row\n\tprint(row)\n  # do something with the row such as cleaning it and writing it to a buffer\n\t# continue requesting and printing data\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">search_twitter</span>(<span class=\"pl-s1\">query</span>):\n\t<span class=\"pl-k\">for</span> <span class=\"pl-s1\">row</span> <span class=\"pl-c1\">in</span> <span class=\"pl-en\">paginated_get</span>(<span class=\"pl-s1\">query</span>):\n\t\t<span class=\"pl-k\">yield</span> <span class=\"pl-s1\">row</span>\n\n<span class=\"pl-c\"># Get one row at a time</span>\n<span class=\"pl-k\">for</span> <span class=\"pl-s1\">row</span> <span class=\"pl-c1\">in</span> <span class=\"pl-en\">extract_data</span>(<span class=\"pl-s\">\"cat pictures\"</span>):\n\t<span class=\"pl-c\"># print the row</span>\n\t<span class=\"pl-en\">print</span>(<span class=\"pl-s1\">row</span>)\n  <span class=\"pl-c\"># do something with the row such as cleaning it and writing it to a buffer</span>\n\t<span class=\"pl-c\"># continue requesting and printing data</span></pre></div>\n<p dir=\"auto\">When calling <code>for row in extract_data(\"cat pictures\"):</code> the function only runs until the first data item is yielded, before printing - so we do not need to wait long for the first value. It will then continue until there is no more data to get.</p>\n<p dir=\"auto\">If we wanted to get all the values at once from a generator instead of one by one, we would need to first ‚Äúrun‚Äù the generator and collect the data. For example, if we wanted to get all the data in memory we could do <code>data = list(extract_data(\"cat pictures\"))</code> which would run the generator and collect all the data in a list before continuing.</p>\n<h2 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-3-extraction-examples\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#3-extraction-examples\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>3 Extraction examples:</h2>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-example-1-grabbing-data-from-an-api\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#example-1-grabbing-data-from-an-api\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Example 1: Grabbing data from an api</h3>\n<blockquote>\n<p dir=\"auto\">üí° This is the bread and butter of data engineers pulling data, so follow along in the colab or in your local setup.</p>\n</blockquote>\n<p dir=\"auto\">For these purposes we created an api that can serve the data you are already familiar with, the NYC taxi dataset.</p>\n<p dir=\"auto\">The api documentation is as follows:</p>\n<ul dir=\"auto\">\n<li>There are a limited nr of records behind the api</li>\n<li>The data can be requested page by page, each page containing 1000 records</li>\n<li>If we request a page with no data, we will get a successful response with no data</li>\n<li>so this means that when we get an empty page, we know there is no more data and we can stop requesting pages - this is a common way to paginate but not the only one - each api may be different.</li>\n<li>details:\n<ul dir=\"auto\">\n<li>method: get</li>\n<li>url: <code>https://us-central1-dlthub-analytics.cloudfunctions.net/data_engineering_zoomcamp_api</code></li>\n<li>parameters: <code>page</code>  integer. Represents the page number you are requesting. Defaults to 1.</li>\n</ul>\n</li>\n</ul>\n<p dir=\"auto\">So how do we design our requester?</p>\n<ul dir=\"auto\">\n<li>We need to request page by page until we get no more data. At this point, we do not know how much data is behind the api.</li>\n<li>It could be 1000 records or it could be 10GB of records. So let‚Äôs grab the data with a generator to avoid having to fit an undetermined amount of data into ram.</li>\n</ul>\n<p dir=\"auto\">In this approach to grabbing data from apis, we have pros and cons:</p>\n<ul dir=\"auto\">\n<li>Pros: <strong>Easy memory management</strong> thanks to api returning events/pages</li>\n<li>Cons: <strong>Low throughput</strong>, due to the data transfer being constrained via an API.</li>\n</ul>\n<div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import requests\n\nBASE_API_URL = &quot;https://us-central1-dlthub-analytics.cloudfunctions.net/data_engineering_zoomcamp_api&quot;\n\n# I call this a paginated getter\n# as it's a function that gets data\n# and also paginates until there is no more data\n# by yielding pages, we &quot;microbatch&quot;, which speeds up downstream processing\n\ndef paginated_getter():\n    page_number = 1\n\n    while True:\n        # Set the query parameters\n        params = {'page': page_number}\n\n        # Make the GET request to the API\n        response = requests.get(BASE_API_URL, params=params)\n        response.raise_for_status()  # Raise an HTTPError for bad responses\n        page_json = response.json()\n        print(f'got page number {page_number} with {len(page_json)} records')\n\n        # if the page has no records, stop iterating\n        if page_json:\n            yield page_json\n            page_number += 1\n        else:\n            # No more data, break the loop\n            break\n\nif __name__ == '__main__':\n    # Use the generator to iterate over pages\n    for page_data in paginated_getter():\n        # Process each page as needed\n        print(page_data)\"><pre>import requests\n\nBASE_API_URL = <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>https://us-central1-dlthub-analytics.cloudfunctions.net/data_engineering_zoomcamp_api<span class=\"pl-pds\">\"</span></span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> I call this a paginated getter</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> as it's a function that gets data</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> and also paginates until there is no more data</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> by yielding pages, we \"microbatch\", which speeds up downstream processing</span>\n\ndef <span class=\"pl-en\">paginated_getter</span>():\n    page_number = 1\n\n    <span class=\"pl-k\">while</span> True:\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Set the query parameters</span>\n        params = {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>page<span class=\"pl-pds\">'</span></span>: page_number}\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Make the GET request to the API</span>\n        response = requests.get(BASE_API_URL, params=params)\n        <span class=\"pl-en\">response.raise_for_status</span>()  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Raise an HTTPError for bad responses</span>\n        page_json = <span class=\"pl-en\">response.json</span>()\n        print(f<span class=\"pl-s\"><span class=\"pl-pds\">'</span>got page number {page_number} with {len(page_json)} records<span class=\"pl-pds\">'</span></span>)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> if the page has no records, stop iterating</span>\n        <span class=\"pl-k\">if</span> page_json:\n            yield page_json\n            page_number += 1\n        else:\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> No more data, break the loop</span>\n            <span class=\"pl-c1\">break</span>\n\n<span class=\"pl-k\">if</span> __name__ == <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Use the generator to iterate over pages</span>\n    <span class=\"pl-k\">for</span> <span class=\"pl-smi\">page_data</span> <span class=\"pl-k\">in</span> <span class=\"pl-en\">paginated_getter</span>():\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Process each page as needed</span>\n        print(page_data)</pre></div>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-example-2-grabbing-the-same-data-from-file---simple-download\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#example-2-grabbing-the-same-data-from-file---simple-download\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Example 2: Grabbing the same data from file - simple download</h3>\n<blockquote>\n<p dir=\"auto\">üí° This part is demonstrative, so you do not need to follow along; just pay attention.</p>\n</blockquote>\n<ul dir=\"auto\">\n<li>Why am I showing you this? so when you do this in the future, you will remember there is a best practice you can apply for scalability.</li>\n</ul>\n<p dir=\"auto\">Some apis respond with files instead of pages of data. The reason for this is simple: Throughput and cost. A restful api that returns data has to read the data from storage and process and return it to you by some logic - If this data is large, this costs time, money and creates a bottleneck.</p>\n<p dir=\"auto\">A better way is to offer the data as files that someone can download from storage directly, without going through the restful api layer. This is common for apis that offer large volumes of data, such as ad impressions data.</p>\n<p dir=\"auto\">In this example, we grab exactly the same data as we did in the API example above, but now we get it from the underlying file instead of going through the API.</p>\n<ul dir=\"auto\">\n<li>Pros: <strong>High throughput</strong></li>\n<li>Cons: <strong>Memory</strong> is used to hold all the data</li>\n</ul>\n<p dir=\"auto\">This is how the code could look. As you can see in this case our <code>data</code>and  <code>parsed_data</code> variables hold the entire file data in memory before returning it. Not great.</p>\n<div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import requests\nimport json\n\nurl = &quot;https://storage.googleapis.com/dtc_zoomcamp_api/yellow_tripdata_2009-06.jsonl&quot;\n\ndef download_and_read_jsonl(url):\n    response = requests.get(url)\n    response.raise_for_status()  # Raise an HTTPError for bad responses\n    data = response.text.splitlines()\n    parsed_data = [json.loads(line) for line in data]\n    return parsed_data\n   \n\ndownloaded_data = download_and_read_jsonl(url)\n\nif downloaded_data:\n    # Process or print the downloaded data as needed\n    print(downloaded_data[:5])  # Print the first 5 entries as an example\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">requests</span>\n<span class=\"pl-k\">import</span> <span class=\"pl-s1\">json</span>\n\n<span class=\"pl-s1\">url</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s\">\"https://storage.googleapis.com/dtc_zoomcamp_api/yellow_tripdata_2009-06.jsonl\"</span>\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">download_and_read_jsonl</span>(<span class=\"pl-s1\">url</span>):\n    <span class=\"pl-s1\">response</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">requests</span>.<span class=\"pl-en\">get</span>(<span class=\"pl-s1\">url</span>)\n    <span class=\"pl-s1\">response</span>.<span class=\"pl-en\">raise_for_status</span>()  <span class=\"pl-c\"># Raise an HTTPError for bad responses</span>\n    <span class=\"pl-s1\">data</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">response</span>.<span class=\"pl-s1\">text</span>.<span class=\"pl-en\">splitlines</span>()\n    <span class=\"pl-s1\">parsed_data</span> <span class=\"pl-c1\">=</span> [<span class=\"pl-s1\">json</span>.<span class=\"pl-en\">loads</span>(<span class=\"pl-s1\">line</span>) <span class=\"pl-k\">for</span> <span class=\"pl-s1\">line</span> <span class=\"pl-c1\">in</span> <span class=\"pl-s1\">data</span>]\n    <span class=\"pl-k\">return</span> <span class=\"pl-s1\">parsed_data</span>\n   \n\n<span class=\"pl-s1\">downloaded_data</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">download_and_read_jsonl</span>(<span class=\"pl-s1\">url</span>)\n\n<span class=\"pl-k\">if</span> <span class=\"pl-s1\">downloaded_data</span>:\n    <span class=\"pl-c\"># Process or print the downloaded data as needed</span>\n    <span class=\"pl-en\">print</span>(<span class=\"pl-s1\">downloaded_data</span>[:<span class=\"pl-c1\">5</span>])  <span class=\"pl-c\"># Print the first 5 entries as an example</span></pre></div>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-example-3-same-file-streaming-download\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#example-3-same-file-streaming-download\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Example 3: Same file, streaming download</h3>\n<blockquote>\n<p dir=\"auto\">üí° This is the bread and butter of data engineers pulling data, so follow along in the colab</p>\n</blockquote>\n<p dir=\"auto\">Ok, downloading files is simple, but what if we want to do a stream download?</p>\n<p dir=\"auto\">That‚Äôs possible too - in effect giving us the best of both worlds. In this case we prepared a jsonl file which is already split into lines making our code simple. But json (not jsonl) files could also be downloaded in this fashion, for example using the <code>ijson</code> library.</p>\n<p dir=\"auto\">What are the pros and cons of this method of grabbing data?</p>\n<p dir=\"auto\">Pros: <strong>High throughput, easy memory management,</strong> because we are downloading a file</p>\n<p dir=\"auto\">Cons: <strong>Difficult to do for columnar file formats</strong>, as entire blocks need to be downloaded before they can be deserialised to rows. Sometimes, the code is complex too.</p>\n<p dir=\"auto\">Here‚Äôs what the code looks like - in a jsonl file each line is a json document, or a ‚Äúrow‚Äù of data, so we yield them as they get downloaded. This allows us to download one row and process it before getting the next row.</p>\n<div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import requests\nimport json\n\ndef download_and_yield_rows(url):\n    response = requests.get(url, stream=True)\n    response.raise_for_status()  # Raise an HTTPError for bad responses\n\n    for line in response.iter_lines():\n        if line:\n            yield json.loads(line)\n\n# Replace the URL with your actual URL\nurl = &quot;https://storage.googleapis.com/dtc_zoomcamp_api/yellow_tripdata_2009-06.jsonl&quot;\n\n# Use the generator to iterate over rows with minimal memory usage\nfor row in download_and_yield_rows(url):\n    # Process each row as needed\n    print(row)\"><pre>import requests\nimport json\n\ndef download_and_yield_rows(url):\n    response = requests.get(url, stream=True)\n    <span class=\"pl-en\">response.raise_for_status</span>()  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Raise an HTTPError for bad responses</span>\n\n    <span class=\"pl-k\">for</span> <span class=\"pl-smi\">line</span> <span class=\"pl-k\">in</span> <span class=\"pl-en\">response.iter_lines</span>():\n        <span class=\"pl-k\">if</span> line:\n            yield json.loads(line)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Replace the URL with your actual URL</span>\nurl = <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>https://storage.googleapis.com/dtc_zoomcamp_api/yellow_tripdata_2009-06.jsonl<span class=\"pl-pds\">\"</span></span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Use the generator to iterate over rows with minimal memory usage</span>\n<span class=\"pl-k\">for</span> <span class=\"pl-smi\">row</span> <span class=\"pl-k\">in</span> download_and_yield_rows(url):\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Process each row as needed</span>\n    print(row)</pre></div>\n<p dir=\"auto\">In the colab notebook you can also find a code snippet to load the data - but we will load some data later in the course and you can explore the colab on your own after the course.</p>\n<p dir=\"auto\">What is worth keeping in mind at this point is that our loader library that we will use later, <code>dlt</code>or data load tool, will respect the streaming concept of the generator and will process it in an efficient way keeping memory usage low and using parallelism where possible.</p>\n<p dir=\"auto\">Let‚Äôs move over to the Colab notebook and run examples 2 and 3, compare them, and finally load examples 1 and 3 to DuckDB</p>\n<h1 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-normalising-data\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#normalising-data\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Normalising data</h1>\n<p dir=\"auto\">You often hear that data people spend most of their time ‚Äúcleaning‚Äù data. What does this mean?</p>\n<p dir=\"auto\">Let‚Äôs look granularly into what people consider data cleaning.</p>\n<p dir=\"auto\">Usually we have 2 parts:</p>\n<ul dir=\"auto\">\n<li>Normalising data without changing its meaning,</li>\n<li>and filtering data for a use case, which changes its meaning.</li>\n</ul>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-part-of-what-we-often-call-data-cleaning-is-just-metadata-work\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#part-of-what-we-often-call-data-cleaning-is-just-metadata-work\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Part of what we often call data cleaning is just metadata work:</h3>\n<ul dir=\"auto\">\n<li>Add types (string to number, string to timestamp, etc)</li>\n<li>Rename columns: Ensure column names follow a supported standard downstream - such as no strange characters in the names.</li>\n<li>Flatten nested dictionaries: Bring nested dictionary values into the top dictionary row</li>\n<li>Unnest lists or arrays into child tables: Arrays or lists cannot be flattened into their parent record, so if we want flat data we need to break them out into separate tables.</li>\n<li>We will look at a practical example next, as these concepts can be difficult to visualise from text.</li>\n</ul>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-why-prepare-data-why-not-use-json-as-is\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#why-prepare-data-why-not-use-json-as-is\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a><strong>Why prepare data? why not use json as is?</strong></h3>\n<ul dir=\"auto\">\n<li>We do not easily know what is inside a json document due to lack of schema</li>\n<li>Types are not enforced between rows of json - we could have one record where age is¬†<code>25</code>and another where age is¬†<code>twenty five</code>¬†, and another where it‚Äôs¬†<code>25.00</code>.  Or in some systems, you might have a dictionary for a single record, but a list of dicts for multiple records. This could easily lead to applications downstream breaking.</li>\n<li>We cannot just use json data easily, for example we would need to convert strings to time if we want to do a daily aggregation.</li>\n<li>Reading json loads more data into memory, as the whole document is scanned - while in parquet or databases we can scan a single column of a document. This causes costs and slowness.</li>\n<li>Json is not fast to aggregate - columnar formats are.</li>\n<li>Json is not fast to search.</li>\n<li>Basically json is designed as a \"lowest common denominator format\" for \"interchange\" / data transfer and is unsuitable for direct analytical usage.</li>\n</ul>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-practical-example\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#practical-example\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Practical example</h3>\n<blockquote>\n<p dir=\"auto\">üí° This is the bread and butter of data engineers pulling data, so follow along in the colab notebook.</p>\n</blockquote>\n<p dir=\"auto\">In the case of the NY taxi rides data, the dataset is quite clean - so let‚Äôs instead use a small example of more complex data. Let‚Äôs assume we know some information about passengers and stops.</p>\n<p dir=\"auto\">For this example we modified the dataset as follows</p>\n<ul dir=\"auto\">\n<li>\n<p dir=\"auto\">We added nested dictionaries</p>\n<div class=\"highlight highlight-source-json notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"&quot;coordinates&quot;: {\n            &quot;start&quot;: {\n                &quot;lon&quot;: -73.787442,\n                &quot;lat&quot;: 40.641525\n                },\"><pre><span class=\"pl-ent\">\"coordinates\"</span>: {\n            <span class=\"pl-ent\">\"start\"</span>: {\n                <span class=\"pl-ent\">\"lon\"</span>: <span class=\"pl-c1\">-73.787442</span>,\n                <span class=\"pl-ent\">\"lat\"</span>: <span class=\"pl-c1\">40.641525</span>\n                },</pre></div>\n</li>\n<li>\n<p dir=\"auto\">We added nested lists</p>\n<div class=\"highlight highlight-source-json notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"&quot;passengers&quot;: [\n            {&quot;name&quot;: &quot;John&quot;, &quot;rating&quot;: 4.9},\n            {&quot;name&quot;: &quot;Jack&quot;, &quot;rating&quot;: 3.9}\n              ],\"><pre><span class=\"pl-ent\">\"passengers\"</span>: [\n            {<span class=\"pl-ent\">\"name\"</span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>John<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-ent\">\"rating\"</span>: <span class=\"pl-c1\">4.9</span>},\n            {<span class=\"pl-ent\">\"name\"</span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Jack<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-ent\">\"rating\"</span>: <span class=\"pl-c1\">3.9</span>}\n              ],</pre></div>\n</li>\n<li>\n<p dir=\"auto\">We added a record hash that gives us an unique id for the record, for easy identification</p>\n<div class=\"highlight highlight-source-json notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"&quot;record_hash&quot;: &quot;b00361a396177a9cb410ff61f20015ad&quot;,\"><pre><span class=\"pl-ent\">\"record_hash\"</span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>b00361a396177a9cb410ff61f20015ad<span class=\"pl-pds\">\"</span></span>,</pre></div>\n</li>\n</ul>\n<p dir=\"auto\">We want to load this data to a database. How do we want to clean the data?</p>\n<ul dir=\"auto\">\n<li>We want to flatten dictionaries into the base row</li>\n<li>We want to flatten lists into a separate table</li>\n<li>We want to convert time strings into time type</li>\n</ul>\n<div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"data = [\n    {\n        &quot;vendor_name&quot;: &quot;VTS&quot;,\n\t\t&quot;record_hash&quot;: &quot;b00361a396177a9cb410ff61f20015ad&quot;,\n        &quot;time&quot;: {\n            &quot;pickup&quot;: &quot;2009-06-14 23:23:00&quot;,\n            &quot;dropoff&quot;: &quot;2009-06-14 23:48:00&quot;\n        },\n        &quot;Trip_Distance&quot;: 17.52,\n        &quot;coordinates&quot;: {\n            &quot;start&quot;: {\n                &quot;lon&quot;: -73.787442,\n                &quot;lat&quot;: 40.641525\n            },\n            &quot;end&quot;: {\n                &quot;lon&quot;: -73.980072,\n                &quot;lat&quot;: 40.742963\n            }\n        },\n        &quot;Rate_Code&quot;: None,\n        &quot;store_and_forward&quot;: None,\n        &quot;Payment&quot;: {\n            &quot;type&quot;: &quot;Credit&quot;,\n            &quot;amt&quot;: 20.5,\n            &quot;surcharge&quot;: 0,\n            &quot;mta_tax&quot;: None,\n            &quot;tip&quot;: 9,\n            &quot;tolls&quot;: 4.15,\n\t\t\t&quot;status&quot;: &quot;booked&quot;\n        },\n        &quot;Passenger_Count&quot;: 2,\n        &quot;passengers&quot;: [\n            {&quot;name&quot;: &quot;John&quot;, &quot;rating&quot;: 4.9},\n            {&quot;name&quot;: &quot;Jack&quot;, &quot;rating&quot;: 3.9}\n        ],\n        &quot;Stops&quot;: [\n            {&quot;lon&quot;: -73.6, &quot;lat&quot;: 40.6},\n            {&quot;lon&quot;: -73.5, &quot;lat&quot;: 40.5}\n        ]\n    },\n]\"><pre><span class=\"pl-s1\">data</span> <span class=\"pl-c1\">=</span> [\n    {\n        <span class=\"pl-s\">\"vendor_name\"</span>: <span class=\"pl-s\">\"VTS\"</span>,\n\t\t<span class=\"pl-s\">\"record_hash\"</span>: <span class=\"pl-s\">\"b00361a396177a9cb410ff61f20015ad\"</span>,\n        <span class=\"pl-s\">\"time\"</span>: {\n            <span class=\"pl-s\">\"pickup\"</span>: <span class=\"pl-s\">\"2009-06-14 23:23:00\"</span>,\n            <span class=\"pl-s\">\"dropoff\"</span>: <span class=\"pl-s\">\"2009-06-14 23:48:00\"</span>\n        },\n        <span class=\"pl-s\">\"Trip_Distance\"</span>: <span class=\"pl-c1\">17.52</span>,\n        <span class=\"pl-s\">\"coordinates\"</span>: {\n            <span class=\"pl-s\">\"start\"</span>: {\n                <span class=\"pl-s\">\"lon\"</span>: <span class=\"pl-c1\">-</span><span class=\"pl-c1\">73.787442</span>,\n                <span class=\"pl-s\">\"lat\"</span>: <span class=\"pl-c1\">40.641525</span>\n            },\n            <span class=\"pl-s\">\"end\"</span>: {\n                <span class=\"pl-s\">\"lon\"</span>: <span class=\"pl-c1\">-</span><span class=\"pl-c1\">73.980072</span>,\n                <span class=\"pl-s\">\"lat\"</span>: <span class=\"pl-c1\">40.742963</span>\n            }\n        },\n        <span class=\"pl-s\">\"Rate_Code\"</span>: <span class=\"pl-c1\">None</span>,\n        <span class=\"pl-s\">\"store_and_forward\"</span>: <span class=\"pl-c1\">None</span>,\n        <span class=\"pl-s\">\"Payment\"</span>: {\n            <span class=\"pl-s\">\"type\"</span>: <span class=\"pl-s\">\"Credit\"</span>,\n            <span class=\"pl-s\">\"amt\"</span>: <span class=\"pl-c1\">20.5</span>,\n            <span class=\"pl-s\">\"surcharge\"</span>: <span class=\"pl-c1\">0</span>,\n            <span class=\"pl-s\">\"mta_tax\"</span>: <span class=\"pl-c1\">None</span>,\n            <span class=\"pl-s\">\"tip\"</span>: <span class=\"pl-c1\">9</span>,\n            <span class=\"pl-s\">\"tolls\"</span>: <span class=\"pl-c1\">4.15</span>,\n\t\t\t<span class=\"pl-s\">\"status\"</span>: <span class=\"pl-s\">\"booked\"</span>\n        },\n        <span class=\"pl-s\">\"Passenger_Count\"</span>: <span class=\"pl-c1\">2</span>,\n        <span class=\"pl-s\">\"passengers\"</span>: [\n            {<span class=\"pl-s\">\"name\"</span>: <span class=\"pl-s\">\"John\"</span>, <span class=\"pl-s\">\"rating\"</span>: <span class=\"pl-c1\">4.9</span>},\n            {<span class=\"pl-s\">\"name\"</span>: <span class=\"pl-s\">\"Jack\"</span>, <span class=\"pl-s\">\"rating\"</span>: <span class=\"pl-c1\">3.9</span>}\n        ],\n        <span class=\"pl-s\">\"Stops\"</span>: [\n            {<span class=\"pl-s\">\"lon\"</span>: <span class=\"pl-c1\">-</span><span class=\"pl-c1\">73.6</span>, <span class=\"pl-s\">\"lat\"</span>: <span class=\"pl-c1\">40.6</span>},\n            {<span class=\"pl-s\">\"lon\"</span>: <span class=\"pl-c1\">-</span><span class=\"pl-c1\">73.5</span>, <span class=\"pl-s\">\"lat\"</span>: <span class=\"pl-c1\">40.5</span>}\n        ]\n    },\n]</pre></div>\n<p dir=\"auto\">Now let‚Äôs normalise this data.</p>\n<h2 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-introducing-dlt\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#introducing-dlt\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Introducing dlt</h2>\n<p dir=\"auto\">dlt is a python library created for the purpose of assisting data engineers to build simpler, faster and more robust pipelines with minimal effort.</p>\n<p dir=\"auto\">You can think of dlt as a loading tool that implements the best practices of data pipelines enabling you to just ‚Äúuse‚Äù those best practices in your own pipelines, in a declarative way.</p>\n<p dir=\"auto\">This enables you to stop reinventing the flat tyre, and leverage dlt to build pipelines much faster than if you did everything from scratch.</p>\n<p dir=\"auto\">dlt automates much of the tedious work a data engineer would do, and does it in a way that is robust. dlt can handle things like:</p>\n<ul dir=\"auto\">\n<li>Schema: Inferring and evolving schema, alerting changes, using schemas as data contracts.</li>\n<li>Typing data, flattening structures, renaming columns to fit database standards.  In our example we will pass the ‚Äúdata‚Äù you can see above and see it normalised.</li>\n<li>Processing a stream of events/rows without filling memory. This includes extraction from generators.</li>\n<li>Loading to a variety of dbs or file formats.</li>\n</ul>\n<p dir=\"auto\">Let‚Äôs use it to load our nested json to duckdb:</p>\n<p dir=\"auto\">Here‚Äôs how you would do that on your local machine. I will walk you through before showing you in colab as well.</p>\n<p dir=\"auto\">First, install dlt</p>\n<div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"# Make sure you are using Python 3.8-3.11 and have pip installed\n# spin up a venv\npython -m venv ./env\nsource ./env/bin/activate\n# pip install\npip install dlt[duckdb]\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> Make sure you are using Python 3.8-3.11 and have pip installed</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> spin up a venv</span>\npython -m venv ./env\n<span class=\"pl-c1\">source</span> ./env/bin/activate\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> pip install</span>\npip install dlt[duckdb]</pre></div>\n<p dir=\"auto\">Next, grab your data from above and run this snippet</p>\n<ul dir=\"auto\">\n<li>here we define a pipeline, which is a connection to a destination</li>\n<li>and we run the pipeline, printing the outcome</li>\n</ul>\n<div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"# define the connection to load to. \n# We now use duckdb, but you can switch to Bigquery later\npipeline = dlt.pipeline(pipeline_name=&quot;taxi_data&quot;,\n\t\t\t\t\t\tdestination='duckdb', \n\t\t\t\t\t\tdataset_name='taxi_rides')\n\n# run the pipeline with default settings, and capture the outcome\ninfo = pipeline.run(data, \n                    table_name=&quot;users&quot;, \n                    write_disposition=&quot;replace&quot;)\n\n# show the outcome\nprint(info)\"><pre><span class=\"pl-c\"># define the connection to load to. </span>\n<span class=\"pl-c\"># We now use duckdb, but you can switch to Bigquery later</span>\n<span class=\"pl-s1\">pipeline</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">dlt</span>.<span class=\"pl-en\">pipeline</span>(<span class=\"pl-s1\">pipeline_name</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">\"taxi_data\"</span>,\n\t\t\t\t\t\t<span class=\"pl-s1\">destination</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">'duckdb'</span>, \n\t\t\t\t\t\t<span class=\"pl-s1\">dataset_name</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">'taxi_rides'</span>)\n\n<span class=\"pl-c\"># run the pipeline with default settings, and capture the outcome</span>\n<span class=\"pl-s1\">info</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">pipeline</span>.<span class=\"pl-en\">run</span>(<span class=\"pl-s1\">data</span>, \n                    <span class=\"pl-s1\">table_name</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">\"users\"</span>, \n                    <span class=\"pl-s1\">write_disposition</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">\"replace\"</span>)\n\n<span class=\"pl-c\"># show the outcome</span>\n<span class=\"pl-en\">print</span>(<span class=\"pl-s1\">info</span>)</pre></div>\n<p dir=\"auto\">If you are running dlt locally you can use the built in streamlit app by running the cli command with the pipeline name we chose above.</p>\n<div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"dlt pipeline taxi_data show\"><pre>dlt pipeline taxi_data show</pre></div>\n<p dir=\"auto\">Or explore the data in the linked colab notebook. I‚Äôll switch to it now to show you the data.</p>\n<h1 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-incremental-loading\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#incremental-loading\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Incremental loading</h1>\n<p dir=\"auto\">Incremental loading means that as we update our datasets with the new data, we would only load the new data, as opposed to making a full copy of a source‚Äôs data all over again and replacing the old version.</p>\n<p dir=\"auto\">By loading incrementally, our pipelines run faster and cheaper.</p>\n<ul dir=\"auto\">\n<li>Incremental loading goes hand in hand with incremental extraction and state, two concepts which we will not delve into during this workshop\n<ul dir=\"auto\">\n<li><code>State</code> is information that keeps track of what was loaded, to know what else remains to be loaded. dlt stores the state at the destination in a separate table.</li>\n<li>Incremental extraction refers to only requesting the increment of data that we need, and not more. This is tightly connected to the state to determine the exact chunk that needs to be extracted and loaded.</li>\n</ul>\n</li>\n<li>You can learn more about incremental extraction and state by reading the dlt docs on how to do it.</li>\n</ul>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-dlt-currently-supports-2-ways-of-loading-incrementally\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#dlt-currently-supports-2-ways-of-loading-incrementally\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>dlt currently supports 2 ways of loading incrementally:</h3>\n<ol dir=\"auto\">\n<li>Append:\n<ul dir=\"auto\">\n<li>We can use this for immutable or stateless events (data that doesn‚Äôt change), such as taxi rides - For example,  every day there are new rides, and we could load the new ones only instead of the entire history.</li>\n<li>We could also use this to load different versions of stateful data, for example for creating a ‚Äúslowly changing dimension‚Äù table for auditing changes. For example, if we load a list of cars and their colors every day, and one day one car changes color, we need both sets of data to be able to discern that a change happened.</li>\n</ul>\n</li>\n<li>Merge:\n<ul dir=\"auto\">\n<li>We can use this to update  data that changes.</li>\n<li>For example, a taxi ride could have a payment status, which is originally ‚Äúbooked‚Äù but could later be changed into ‚Äúpaid‚Äù, ‚Äúrejected‚Äù or ‚Äúcancelled‚Äù</li>\n</ul>\n</li>\n</ol>\n<p dir=\"auto\">Here is how you can think about which method to use:</p>\n<p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/DataTalksClub/data-engineering-zoomcamp/blob/main/cohorts/2024/workshops/dlt_resources/incremental_loading.png\"><img src=\"/DataTalksClub/data-engineering-zoomcamp/raw/main/cohorts/2024/workshops/dlt_resources/incremental_loading.png\" alt=\"Incremental Loading\" style=\"max-width: 100%;\"></a></p>\n<ul dir=\"auto\">\n<li>If you want to keep track of when changes occur in stateful data (slowly changing dimension) then you will need to append the data</li>\n</ul>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-lets-do-a-merge-example-together\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#lets-do-a-merge-example-together\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>Let‚Äôs do a merge example together:</h3>\n<blockquote>\n<p dir=\"auto\">üí° This is the bread and butter of data engineers pulling data, so follow along.</p>\n</blockquote>\n<ul dir=\"auto\">\n<li>In our previous example, the payment status changed from \"booked\" to ‚Äúcancelled‚Äù. Perhaps Jack likes to fraud taxis and that explains his low rating. Besides the ride status change, he also got his rating lowered further.</li>\n<li>The merge operation replaces an old record with a new one based on a key. The key could consist of multiple fields or a single unique id. We will use record hash that we created for simplicity. If you do not have a unique key, you could create one deterministically out of several fields, such as by concatenating the data and hashing it.</li>\n<li>A merge operation replaces rows, it does not update them. If you want to update only parts of a row, you would have to load the new data by appending it and doing a custom transformation to combine the old and new data.</li>\n</ul>\n<p dir=\"auto\">In this example, the score of the 2 drivers got lowered and we need to update the values. We do it by using merge write disposition, replacing the records identified by  <code>record hash</code> present in the new data.</p>\n<div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"data = [\n    {\n        &quot;vendor_name&quot;: &quot;VTS&quot;,\n\t\t&quot;record_hash&quot;: &quot;b00361a396177a9cb410ff61f20015ad&quot;,\n        &quot;time&quot;: {\n            &quot;pickup&quot;: &quot;2009-06-14 23:23:00&quot;,\n            &quot;dropoff&quot;: &quot;2009-06-14 23:48:00&quot;\n        },\n        &quot;Trip_Distance&quot;: 17.52,\n        &quot;coordinates&quot;: {\n            &quot;start&quot;: {\n                &quot;lon&quot;: -73.787442,\n                &quot;lat&quot;: 40.641525\n            },\n            &quot;end&quot;: {\n                &quot;lon&quot;: -73.980072,\n                &quot;lat&quot;: 40.742963\n            }\n        },\n        &quot;Rate_Code&quot;: None,\n        &quot;store_and_forward&quot;: None,\n        &quot;Payment&quot;: {\n            &quot;type&quot;: &quot;Credit&quot;,\n            &quot;amt&quot;: 20.5,\n            &quot;surcharge&quot;: 0,\n            &quot;mta_tax&quot;: None,\n            &quot;tip&quot;: 9,\n            &quot;tolls&quot;: 4.15,\n\t\t\t&quot;status&quot;: &quot;cancelled&quot;\n        },\n        &quot;Passenger_Count&quot;: 2,\n        &quot;passengers&quot;: [\n            {&quot;name&quot;: &quot;John&quot;, &quot;rating&quot;: 4.4},\n            {&quot;name&quot;: &quot;Jack&quot;, &quot;rating&quot;: 3.6}\n        ],\n        &quot;Stops&quot;: [\n            {&quot;lon&quot;: -73.6, &quot;lat&quot;: 40.6},\n            {&quot;lon&quot;: -73.5, &quot;lat&quot;: 40.5}\n        ]\n    },\n]\n\n# define the connection to load to. \n# We now use duckdb, but you can switch to Bigquery later\npipeline = dlt.pipeline(destination='duckdb', dataset_name='taxi_rides')\n\n# run the pipeline with default settings, and capture the outcome\ninfo = pipeline.run(data, \n\t\t\t\t\ttable_name=&quot;users&quot;, \n\t\t\t\t\twrite_disposition=&quot;merge&quot;, \n\t\t\t\t\tmerge_key=&quot;record_hash&quot;)\n\n# show the outcome\nprint(info)\"><pre><span class=\"pl-s1\">data</span> <span class=\"pl-c1\">=</span> [\n    {\n        <span class=\"pl-s\">\"vendor_name\"</span>: <span class=\"pl-s\">\"VTS\"</span>,\n\t\t<span class=\"pl-s\">\"record_hash\"</span>: <span class=\"pl-s\">\"b00361a396177a9cb410ff61f20015ad\"</span>,\n        <span class=\"pl-s\">\"time\"</span>: {\n            <span class=\"pl-s\">\"pickup\"</span>: <span class=\"pl-s\">\"2009-06-14 23:23:00\"</span>,\n            <span class=\"pl-s\">\"dropoff\"</span>: <span class=\"pl-s\">\"2009-06-14 23:48:00\"</span>\n        },\n        <span class=\"pl-s\">\"Trip_Distance\"</span>: <span class=\"pl-c1\">17.52</span>,\n        <span class=\"pl-s\">\"coordinates\"</span>: {\n            <span class=\"pl-s\">\"start\"</span>: {\n                <span class=\"pl-s\">\"lon\"</span>: <span class=\"pl-c1\">-</span><span class=\"pl-c1\">73.787442</span>,\n                <span class=\"pl-s\">\"lat\"</span>: <span class=\"pl-c1\">40.641525</span>\n            },\n            <span class=\"pl-s\">\"end\"</span>: {\n                <span class=\"pl-s\">\"lon\"</span>: <span class=\"pl-c1\">-</span><span class=\"pl-c1\">73.980072</span>,\n                <span class=\"pl-s\">\"lat\"</span>: <span class=\"pl-c1\">40.742963</span>\n            }\n        },\n        <span class=\"pl-s\">\"Rate_Code\"</span>: <span class=\"pl-c1\">None</span>,\n        <span class=\"pl-s\">\"store_and_forward\"</span>: <span class=\"pl-c1\">None</span>,\n        <span class=\"pl-s\">\"Payment\"</span>: {\n            <span class=\"pl-s\">\"type\"</span>: <span class=\"pl-s\">\"Credit\"</span>,\n            <span class=\"pl-s\">\"amt\"</span>: <span class=\"pl-c1\">20.5</span>,\n            <span class=\"pl-s\">\"surcharge\"</span>: <span class=\"pl-c1\">0</span>,\n            <span class=\"pl-s\">\"mta_tax\"</span>: <span class=\"pl-c1\">None</span>,\n            <span class=\"pl-s\">\"tip\"</span>: <span class=\"pl-c1\">9</span>,\n            <span class=\"pl-s\">\"tolls\"</span>: <span class=\"pl-c1\">4.15</span>,\n\t\t\t<span class=\"pl-s\">\"status\"</span>: <span class=\"pl-s\">\"cancelled\"</span>\n        },\n        <span class=\"pl-s\">\"Passenger_Count\"</span>: <span class=\"pl-c1\">2</span>,\n        <span class=\"pl-s\">\"passengers\"</span>: [\n            {<span class=\"pl-s\">\"name\"</span>: <span class=\"pl-s\">\"John\"</span>, <span class=\"pl-s\">\"rating\"</span>: <span class=\"pl-c1\">4.4</span>},\n            {<span class=\"pl-s\">\"name\"</span>: <span class=\"pl-s\">\"Jack\"</span>, <span class=\"pl-s\">\"rating\"</span>: <span class=\"pl-c1\">3.6</span>}\n        ],\n        <span class=\"pl-s\">\"Stops\"</span>: [\n            {<span class=\"pl-s\">\"lon\"</span>: <span class=\"pl-c1\">-</span><span class=\"pl-c1\">73.6</span>, <span class=\"pl-s\">\"lat\"</span>: <span class=\"pl-c1\">40.6</span>},\n            {<span class=\"pl-s\">\"lon\"</span>: <span class=\"pl-c1\">-</span><span class=\"pl-c1\">73.5</span>, <span class=\"pl-s\">\"lat\"</span>: <span class=\"pl-c1\">40.5</span>}\n        ]\n    },\n]\n\n<span class=\"pl-c\"># define the connection to load to. </span>\n<span class=\"pl-c\"># We now use duckdb, but you can switch to Bigquery later</span>\n<span class=\"pl-s1\">pipeline</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">dlt</span>.<span class=\"pl-en\">pipeline</span>(<span class=\"pl-s1\">destination</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">'duckdb'</span>, <span class=\"pl-s1\">dataset_name</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">'taxi_rides'</span>)\n\n<span class=\"pl-c\"># run the pipeline with default settings, and capture the outcome</span>\n<span class=\"pl-s1\">info</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">pipeline</span>.<span class=\"pl-en\">run</span>(<span class=\"pl-s1\">data</span>, \n\t\t\t\t\t<span class=\"pl-s1\">table_name</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">\"users\"</span>, \n\t\t\t\t\t<span class=\"pl-s1\">write_disposition</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">\"merge\"</span>, \n\t\t\t\t\t<span class=\"pl-s1\">merge_key</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">\"record_hash\"</span>)\n\n<span class=\"pl-c\"># show the outcome</span>\n<span class=\"pl-en\">print</span>(<span class=\"pl-s1\">info</span>)</pre></div>\n<p dir=\"auto\">As you can see in your notebook, the payment status and Jack‚Äôs rating were updated after running the code.</p>\n<h3 tabindex=\"-1\" dir=\"auto\"><a id=\"user-content-whats-next\" class=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" href=\"#whats-next\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a>What‚Äôs next?</h3>\n<ul dir=\"auto\">\n<li>You could change the destination to parquet + local file system or storage bucket. See the colab bonus section.</li>\n<li>You could change the destination to BigQuery. Destination &amp; credential setup docs: <a href=\"https://dlthub.com/docs/dlt-ecosystem/destinations/\" rel=\"nofollow\">https://dlthub.com/docs/dlt-ecosystem/destinations/</a>, <a href=\"https://dlthub.com/docs/walkthroughs/add_credentials\" rel=\"nofollow\">https://dlthub.com/docs/walkthroughs/add_credentials</a>\nor See the colab bonus section.</li>\n<li>You could use a decorator to convert the generator into a customised dlt resource: <a href=\"https://dlthub.com/docs/general-usage/resource\" rel=\"nofollow\">https://dlthub.com/docs/general-usage/resource</a></li>\n<li>You can deep dive into building more complex pipelines by following the guides:\n<ul dir=\"auto\">\n<li><a href=\"https://dlthub.com/docs/walkthroughs\" rel=\"nofollow\">https://dlthub.com/docs/walkthroughs</a></li>\n<li><a href=\"https://dlthub.com/docs/build-a-pipeline-tutorial\" rel=\"nofollow\">https://dlthub.com/docs/build-a-pipeline-tutorial</a></li>\n</ul>\n</li>\n<li>You can join our <a href=\"https://dlthub.com/community\" rel=\"nofollow\">Slack community</a> and engage with us there.</li>\n</ul>\n</article>","renderedFileInfo":null,"shortPath":null,"symbolsEnabled":true,"tabSize":8,"topBannersInfo":{"overridingGlobalFundingFile":false,"globalPreferredFundingPath":null,"repoOwner":"DataTalksClub","repoName":"data-engineering-zoomcamp","showInvalidCitationWarning":false,"citationHelpUrl":"https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/creating-a-repository-on-github/about-citation-files","showDependabotConfigurationBanner":false,"actionsOnboardingTip":null},"truncated":false,"viewable":true,"workflowRedirectUrl":null,"symbols":{"timed_out":false,"not_analyzed":false,"symbols":[{"name":"Intro","kind":"section_1","ident_start":2,"ident_end":7,"extent_start":0,"extent_end":2177,"fully_qualified_name":"Intro","ident_utf16":{"start":{"line_number":0,"utf16_col":2},"end":{"line_number":0,"utf16_col":7}},"extent_utf16":{"start":{"line_number":0,"utf16_col":0},"end":{"line_number":35,"utf16_col":0}}},{"name":"‚ÄúA wild dataset magically appears!‚Äù","kind":"section_3","ident_start":254,"ident_end":293,"extent_start":250,"extent_end":943,"fully_qualified_name":"‚ÄúA wild dataset magically appears!‚Äù","ident_utf16":{"start":{"line_number":6,"utf16_col":4},"end":{"line_number":6,"utf16_col":39}},"extent_utf16":{"start":{"line_number":6,"utf16_col":0},"end":{"line_number":18,"utf16_col":0}}},{"name":"Be the magician! üòé","kind":"section_3","ident_start":947,"ident_end":968,"extent_start":943,"extent_end":1298,"fully_qualified_name":"Be the magician! üòé","ident_utf16":{"start":{"line_number":18,"utf16_col":4},"end":{"line_number":18,"utf16_col":23}},"extent_utf16":{"start":{"line_number":18,"utf16_col":0},"end":{"line_number":28,"utf16_col":0}}},{"name":"What else does a data engineer do? What are we not learning, and what are we learning?","kind":"section_3","ident_start":1302,"ident_end":1388,"extent_start":1298,"extent_end":2177,"fully_qualified_name":"What else does a data engineer do? What are we not learning, and what are we learning?","ident_utf16":{"start":{"line_number":28,"utf16_col":4},"end":{"line_number":28,"utf16_col":90}},"extent_utf16":{"start":{"line_number":28,"utf16_col":0},"end":{"line_number":35,"utf16_col":0}}},{"name":"Extracting data","kind":"section_1","ident_start":2179,"ident_end":2194,"extent_start":2177,"extent_end":15323,"fully_qualified_name":"Extracting data","ident_utf16":{"start":{"line_number":35,"utf16_col":2},"end":{"line_number":35,"utf16_col":17}},"extent_utf16":{"start":{"line_number":35,"utf16_col":0},"end":{"line_number":300,"utf16_col":0}}},{"name":"The considerations of extracting data","kind":"section_3","ident_start":2200,"ident_end":2237,"extent_start":2196,"extent_end":3964,"fully_qualified_name":"The considerations of extracting data","ident_utf16":{"start":{"line_number":37,"utf16_col":4},"end":{"line_number":37,"utf16_col":41}},"extent_utf16":{"start":{"line_number":37,"utf16_col":0},"end":{"line_number":56,"utf16_col":0}}},{"name":"Extracting data without hitting hardware limits","kind":"section_3","ident_start":3968,"ident_end":4015,"extent_start":3964,"extent_end":4218,"fully_qualified_name":"Extracting data without hitting hardware limits","ident_utf16":{"start":{"line_number":56,"utf16_col":4},"end":{"line_number":56,"utf16_col":51}},"extent_utf16":{"start":{"line_number":56,"utf16_col":0},"end":{"line_number":60,"utf16_col":0}}},{"name":"**Managing memory.**","kind":"section_3","ident_start":4222,"ident_end":4242,"extent_start":4218,"extent_end":4982,"fully_qualified_name":"**Managing memory.**","ident_utf16":{"start":{"line_number":60,"utf16_col":4},"end":{"line_number":60,"utf16_col":24}},"extent_utf16":{"start":{"line_number":60,"utf16_col":0},"end":{"line_number":66,"utf16_col":0}}},{"name":"So how do we avoid filling the memory?","kind":"section_3","ident_start":4986,"ident_end":5024,"extent_start":4982,"extent_end":5203,"fully_qualified_name":"So how do we avoid filling the memory?","ident_utf16":{"start":{"line_number":66,"utf16_col":4},"end":{"line_number":66,"utf16_col":42}},"extent_utf16":{"start":{"line_number":66,"utf16_col":0},"end":{"line_number":72,"utf16_col":0}}},{"name":"Control the max memory used by streaming the data","kind":"section_3","ident_start":5207,"ident_end":5256,"extent_start":5203,"extent_end":5920,"fully_qualified_name":"Control the max memory used by streaming the data","ident_utf16":{"start":{"line_number":72,"utf16_col":4},"end":{"line_number":72,"utf16_col":53}},"extent_utf16":{"start":{"line_number":72,"utf16_col":0},"end":{"line_number":90,"utf16_col":0}}},{"name":"Streaming in python via generators","kind":"section_3","ident_start":5924,"ident_end":5958,"extent_start":5920,"extent_end":6739,"fully_qualified_name":"Streaming in python via generators","ident_utf16":{"start":{"line_number":90,"utf16_col":4},"end":{"line_number":90,"utf16_col":38}},"extent_utf16":{"start":{"line_number":90,"utf16_col":0},"end":{"line_number":102,"utf16_col":0}}},{"name":"Generator examples:","kind":"section_3","ident_start":6743,"ident_end":6762,"extent_start":6739,"extent_end":8637,"fully_qualified_name":"Generator examples:","ident_utf16":{"start":{"line_number":102,"utf16_col":4},"end":{"line_number":102,"utf16_col":23}},"extent_utf16":{"start":{"line_number":102,"utf16_col":0},"end":{"line_number":147,"utf16_col":0}}},{"name":"3 Extraction examples:","kind":"section_2","ident_start":8640,"ident_end":8662,"extent_start":8637,"extent_end":15323,"fully_qualified_name":"3 Extraction examples:","ident_utf16":{"start":{"line_number":147,"utf16_col":3},"end":{"line_number":147,"utf16_col":25}},"extent_utf16":{"start":{"line_number":147,"utf16_col":0},"end":{"line_number":300,"utf16_col":0}}},{"name":"Example 1: Grabbing data from an api","kind":"section_3","ident_start":8668,"ident_end":8704,"extent_start":8664,"extent_end":11316,"fully_qualified_name":"Example 1: Grabbing data from an api","ident_utf16":{"start":{"line_number":149,"utf16_col":4},"end":{"line_number":149,"utf16_col":40}},"extent_utf16":{"start":{"line_number":149,"utf16_col":0},"end":{"line_number":216,"utf16_col":0}}},{"name":"Example 2: Grabbing the same data from file - simple download","kind":"section_3","ident_start":11320,"ident_end":11381,"extent_start":11316,"extent_end":13129,"fully_qualified_name":"Example 2: Grabbing the same data from file - simple download","ident_utf16":{"start":{"line_number":216,"utf16_col":4},"end":{"line_number":216,"utf16_col":65}},"extent_utf16":{"start":{"line_number":216,"utf16_col":0},"end":{"line_number":256,"utf16_col":0}}},{"name":"Example 3: Same file, streaming download","kind":"section_3","ident_start":13133,"ident_end":13173,"extent_start":13129,"extent_end":15323,"fully_qualified_name":"Example 3: Same file, streaming download","ident_utf16":{"start":{"line_number":256,"utf16_col":4},"end":{"line_number":256,"utf16_col":44}},"extent_utf16":{"start":{"line_number":256,"utf16_col":0},"end":{"line_number":300,"utf16_col":0}}},{"name":"Normalising data","kind":"section_1","ident_start":15325,"ident_end":15341,"extent_start":15323,"extent_end":21870,"fully_qualified_name":"Normalising data","ident_utf16":{"start":{"line_number":300,"utf16_col":2},"end":{"line_number":300,"utf16_col":18}},"extent_utf16":{"start":{"line_number":300,"utf16_col":0},"end":{"line_number":475,"utf16_col":0}}},{"name":"Part of what we often call data cleaning is just metadata work:","kind":"section_3","ident_start":15656,"ident_end":15719,"extent_start":15652,"extent_end":16273,"fully_qualified_name":"Part of what we often call data cleaning is just metadata work:","ident_utf16":{"start":{"line_number":311,"utf16_col":4},"end":{"line_number":311,"utf16_col":67}},"extent_utf16":{"start":{"line_number":311,"utf16_col":0},"end":{"line_number":319,"utf16_col":0}}},{"name":"**Why prepare data? why not use json as is?**","kind":"section_3","ident_start":16277,"ident_end":16322,"extent_start":16273,"extent_end":17293,"fully_qualified_name":"**Why prepare data? why not use json as is?**","ident_utf16":{"start":{"line_number":319,"utf16_col":4},"end":{"line_number":319,"utf16_col":49}},"extent_utf16":{"start":{"line_number":319,"utf16_col":0},"end":{"line_number":329,"utf16_col":0}}},{"name":"Practical example","kind":"section_3","ident_start":17297,"ident_end":17314,"extent_start":17293,"extent_end":19574,"fully_qualified_name":"Practical example","ident_utf16":{"start":{"line_number":329,"utf16_col":4},"end":{"line_number":329,"utf16_col":21}},"extent_utf16":{"start":{"line_number":329,"utf16_col":0},"end":{"line_number":416,"utf16_col":0}}},{"name":"Introducing dlt","kind":"section_2","ident_start":19577,"ident_end":19592,"extent_start":19574,"extent_end":21870,"fully_qualified_name":"Introducing dlt","ident_utf16":{"start":{"line_number":416,"utf16_col":3},"end":{"line_number":416,"utf16_col":18}},"extent_utf16":{"start":{"line_number":416,"utf16_col":0},"end":{"line_number":475,"utf16_col":0}}},{"name":"Incremental loading","kind":"section_1","ident_start":21872,"ident_end":21891,"extent_start":21870,"extent_end":27342,"fully_qualified_name":"Incremental loading","ident_utf16":{"start":{"line_number":475,"utf16_col":2},"end":{"line_number":475,"utf16_col":21}},"extent_utf16":{"start":{"line_number":475,"utf16_col":0},"end":{"line_number":582,"utf16_col":0}}},{"name":"dlt currently supports 2 ways of loading incrementally:","kind":"section_3","ident_start":22795,"ident_end":22850,"extent_start":22791,"extent_end":23892,"fully_qualified_name":"dlt currently supports 2 ways of loading incrementally:","ident_utf16":{"start":{"line_number":486,"utf16_col":4},"end":{"line_number":486,"utf16_col":59}},"extent_utf16":{"start":{"line_number":486,"utf16_col":0},"end":{"line_number":501,"utf16_col":0}}},{"name":"Let‚Äôs do a merge example together:","kind":"section_3","ident_start":23896,"ident_end":23932,"extent_start":23892,"extent_end":26580,"fully_qualified_name":"Let‚Äôs do a merge example together:","ident_utf16":{"start":{"line_number":501,"utf16_col":4},"end":{"line_number":501,"utf16_col":38}},"extent_utf16":{"start":{"line_number":501,"utf16_col":0},"end":{"line_number":572,"utf16_col":0}}},{"name":"What‚Äôs next?","kind":"section_3","ident_start":26584,"ident_end":26598,"extent_start":26580,"extent_end":27342,"fully_qualified_name":"What‚Äôs next?","ident_utf16":{"start":{"line_number":572,"utf16_col":4},"end":{"line_number":572,"utf16_col":16}},"extent_utf16":{"start":{"line_number":572,"utf16_col":0},"end":{"line_number":582,"utf16_col":0}}}]}},"copilotInfo":null,"copilotAccessAllowed":false,"csrf_tokens":{"/DataTalksClub/data-engineering-zoomcamp/branches":{"post":"bxcajcmV5nS5qdRft_V6mEKxujKsQybCSfSoCM8672M3fuM3ozBw9ZQhHzcApY8F_ikmZ8o_A33PqtNTsQBGFQ"},"/repos/preferences":{"post":"OCUZG-2BhFnXn2WqO3M4CAU-U6xpkX4Gt3w8T4p-6dmZu2D1iROlI3IUdG09abGHqIq5triRknpgv1Ps8rZaKg"}}},"title":"data-engineering-zoomcamp/cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md at main ¬∑ DataTalksClub/data-engineering-zoomcamp"}